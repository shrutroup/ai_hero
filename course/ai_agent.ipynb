{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92fa259-7a02-4c67-bb3f-4c9732163cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutroup/git_repos/ai_hero/course/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063edce2-b284-4a9d-9ff6-d05a919f5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab3e648-ae3f-41c3-80fb-bc461df7b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "#dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "#print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afaf053-d225-475d-88a1-6a10c82c4bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21712"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evidently_docs[45]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a37d97-9b80-4cbf-aee4-adb26fb46bc7",
   "metadata": {},
   "source": [
    "## Simple Splitting - by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae0473c-3991-4497-8e58-8a508fb4b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <=0 or step <=0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "#doc_content = evidently_docs[45].pop('content')\n",
    "chunked_seq = sliding_window(seq=evidently_docs[45]['content'], size=2000, step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a43e920c-bfbd-4eab-9897-992077db36bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"},\n",
       " {'start': 2000,\n",
       "  'chunk': 'e.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'ort WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.'},\n",
       " {'start': 4000,\n",
       "  'chunk': ' is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_f'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'ormats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we s'},\n",
       " {'start': 7000,\n",
       "  'chunk': ' lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use '},\n",
       " {'start': 8000,\n",
       "  'chunk': 'ee different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For '},\n",
       " {'start': 10000,\n",
       "  'chunk': '\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n    '},\n",
       " {'start': 11000,\n",
       "  'chunk': 'an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provide'},\n",
       " {'start': 12000,\n",
       "  'chunk': '    non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'r = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow i'},\n",
       " {'start': 14000,\n",
       "  'chunk': '\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'n the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` '},\n",
       " {'start': 16000,\n",
       "  'chunk': ' Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance betwe'},\n",
       " {'start': 17000,\n",
       "  'chunk': 'to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefin'},\n",
       " {'start': 18000,\n",
       "  'chunk': 'en the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as'},\n",
       " {'start': 19000,\n",
       "  'chunk': 'ition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatical'},\n",
       " {'start': 20000,\n",
       "  'chunk': ' code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3479a249-5450-44ed-aaab-bde6ae6e21eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/r'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ntly/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'eleases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-0'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently'},\n",
       " {'start': 4000,\n",
       "  'chunk': '2-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently v0.6\">\\n  ## **New API release**\\n\\n  The new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently Cloud\">\\n  ## **Editable datasets**\\n\\n  You can now hit \"edit\" on any existing dataset, create a copy and add / delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\\n\\n  ![](/images/changelog/editable_dataset-min.png)\\n</Update>\\n\\n<Update label=\"2025-01-10\" description=\"Docs\">\\n  ## **New Docs**\\n\\n  We are creating a new Docs website in anticipation of API change. You can still access old docs for information on earlier API and examples.\\n</Update>'},\n",
       " {'start': 0,\n",
       "  'chunk': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to '},\n",
       " {'start': 1000,\n",
       "  'chunk': 'on\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/d'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated M'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'escriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping r'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunked_docs = []\n",
    "for i in range(len(evidently_docs)):\n",
    "    chunked_seq = sliding_window(seq=evidently_docs[i]['content'], size=2000, step=1000)\n",
    "    \n",
    "    all_chunked_docs.extend(chunked_seq)\n",
    "\n",
    "all_chunked_docs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f42da40-aabd-4c1b-91f7-8389d54d0d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eed785-f560-4c39-8ced-29dfc7294d4a",
   "metadata": {},
   "source": [
    "## Splitting my Paragraphs and Sections\n",
    "Using newline to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef48b2b3-9bd8-4199-93fd-0384ab0ebbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this tutorial, you will learn how to perform regression testing for LLM outputs.',\n",
       " 'You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.',\n",
       " \"<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\",\n",
       " '# Tutorial scope',\n",
       " \"Here's what we'll do:\",\n",
       " '* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.',\n",
       " '* **Get new answers**. Imitate generating new answers to the same question.',\n",
       " '* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.',\n",
       " '* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.',\n",
       " \"<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\",\n",
       " 'To complete the tutorial, you will need:',\n",
       " '* Basic Python knowledge.\\xa0',\n",
       " '* An OpenAI API key to use for the LLM evaluator.',\n",
       " '* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.',\n",
       " '<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>',\n",
       " '## 1. Installation and Imports',\n",
       " 'Install Evidently:',\n",
       " '```python\\npip install evidently[llm] \\n```',\n",
       " 'Import the required modules:',\n",
       " '```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *',\n",
       " 'from evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```',\n",
       " 'To connect to Evidently Cloud:',\n",
       " '```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```',\n",
       " '**Optional.** To create monitoring panels as code:',\n",
       " '```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```',\n",
       " 'Pass your OpenAI key:',\n",
       " '```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project',\n",
       " 'Connect to Evidently Cloud. Replace with your actual token:',\n",
       " '```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```',\n",
       " 'Create a Project:',\n",
       " '```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset',\n",
       " 'Create a toy dataset with questions and reference answers.&#x20;',\n",
       " '```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]',\n",
       " 'columns = [\"question\", \"target_response\"]',\n",
       " 'ref_data = pd.DataFrame(data, columns=columns)\\n```',\n",
       " 'Get a quick preview:',\n",
       " \"```python\\npd.set_option('display.max_colwidth', None)\\nref_data.head()\\n```\",\n",
       " 'Here is how the data looks:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_preview-min.png)',\n",
       " \"**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let's check the text length and sentence count distribution.\",\n",
       " '```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```',\n",
       " 'In this code, you:',\n",
       " '* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).',\n",
       " '* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).',\n",
       " '* Exported results as a dataframe.',\n",
       " 'Here is the preview:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_stats-min.png)',\n",
       " 'In a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.',\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n])',\n",
       " 'my_eval = report.run(ref_dataset, None)\\nmy_eval',\n",
       " '#my_eval.as_dict()\\n#my_eval.json()\\n```',\n",
       " 'This renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).',\n",
       " '![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers',\n",
       " 'Suppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.',\n",
       " '  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],',\n",
       " '    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],',\n",
       " '    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],',\n",
       " '    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],',\n",
       " '    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Here is the resulting dataset with the added new column:',\n",
       " '![](/images/examples/llm_regression_tutorial_new_data-min.png)',\n",
       " '<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite',\n",
       " 'To compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.',\n",
       " 'Let’s formulate what we want to Tests:',\n",
       " '* **Length check**. All new responses must be no longer than 200 symbols.',\n",
       " '* **Correctness**. All new responses should not contradict the reference answer.',\n",
       " '* **Style**. All new responses should match the style of the reference.',\n",
       " \"Text length is easy to check, but for Correctness and Style, we'll write our custom LLM judges.\",\n",
       " '### Correctness judge',\n",
       " 'We implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.',\n",
       " '<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>',\n",
       " '```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'We recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.',\n",
       " '<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>',\n",
       " '<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>',\n",
       " '### Style judge',\n",
       " \"Using a similar approach, we'll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\",\n",
       " '```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.',\n",
       " 'Consider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.',\n",
       " 'You must focus only on STYLE. Ignore any differences in contents.',\n",
       " '=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'This could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.',\n",
       " 'At the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation',\n",
       " 'Now, we can run tests that evaluate for correctness, style and text length. We do this in two steps.',\n",
       " '**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.',\n",
       " \"We'll include the two evaluators we just created, and built-in `TextLength()` descriptor.\",\n",
       " '```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```',\n",
       " '<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>',\n",
       " 'To add these descriptors to the dataset, run:',\n",
       " '```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```',\n",
       " 'To preview the results of this step locally:',\n",
       " '```python\\neval_dataset.as_dataframe()\\n```',\n",
       " '![](/images/examples/llm_regression_tutorial_scored-min.png)',\n",
       " 'However, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.',\n",
       " \"**Create a Report**. Let's formulate the Report:\",\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```',\n",
       " 'What happens in this code:',\n",
       " '* We create an Evidently Report to compute aggregate Metrics.',\n",
       " '* We use `TextEvals` to summarize all descriptors.',\n",
       " '* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).',\n",
       " '* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).',\n",
       " '* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.',\n",
       " '<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>',\n",
       " \"**Run the Report**. Now that our Report with its test conditions is ready - let's run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\",\n",
       " '```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```',\n",
       " \"Including data is optional but useful for most LLM use cases since you'd want to see not just the aggregate results but also the raw texts outputs.\",\n",
       " '<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>',\n",
       " \"To view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you'll see the Report you can explore.\",\n",
       " 'The Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.',\n",
       " 'Report view, with \"Style\" metric selected:',\n",
       " '![](/images/examples/llm_regression_tutorial_report1-min.png)',\n",
       " '**Note**: your explanations will vary since LLMs are non-deterministic.',\n",
       " 'The Test Suite with all Test results:&#x20;',\n",
       " '![](/images/examples/llm_regression_tutorial_tests1-min.png)',\n",
       " 'You can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.',\n",
       " '<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again',\n",
       " \"Let's say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\",\n",
       " 'Here is the toy `eval_data_2` to imitate the result of the change.',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],',\n",
       " '      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],',\n",
       " '      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.',\n",
       " '      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],',\n",
       " '      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Create a new dataset:',\n",
       " '```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```',\n",
       " '**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:',\n",
       " '```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```',\n",
       " '**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.',\n",
       " '![](/images/examples/llm_regression_tutorial_tests2-min.png)',\n",
       " 'There is also a \"softer\" fail for one of the responses that now has a different tone.',\n",
       " '![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard',\n",
       " 'As you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;',\n",
       " \"Let's create a couple of Panels using Dashboards as code approach so that it's easy to reproduce. The following code will add:\",\n",
       " '* A counter panel to show the SUCCESS rate of the latest Test run.',\n",
       " '* A test monitoring panel to show all Test results over time.',\n",
       " '```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```',\n",
       " 'When you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.',\n",
       " '![](/images/examples/llm_regression_tutorial_dashboard-min.png)',\n",
       " 'If you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.',\n",
       " '<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>',\n",
       " \"**What's next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20c472-6a83-401e-bd3c-5e069df493a7",
   "metadata": {},
   "source": [
    "## Splitting by Sections\n",
    "Taking advantage of the documents structure. Markdown documents have this structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccd0b6a-3a87-430b-9c02-ca56a6c9631d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'^(#{2} )(.+)$', re.MULTILINE|re.UNICODE)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "level=2\n",
    "text = '## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```'\n",
    "header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6755632-77b4-4da0-8dd7-b7ed95e45010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Installation and Imports\n",
      "## \n",
      "1. Installation and Imports\n"
     ]
    }
   ],
   "source": [
    "match = re.search(pattern, text)\n",
    "print(match.group(0))\n",
    "print(match.group(1))\n",
    "print(match.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df991eb-f4dc-499f-8045-88e3028f4b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '## ',\n",
       " '1. Installation and Imports',\n",
       " '\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split and keep the headers\n",
    "parts = pattern.split(text)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50529de1-b58a-498e-ac65-dcf60c6b0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Split markdown text by a specific header level.\n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "        \n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e79fe8e-6fd2-4e7a-8a38-06a49d14700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = split_markdown_by_level(evidently_docs[45]['content'], level=2)\n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57d79668-b3b7-4ce5-95d3-e2d5a0bed4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all the whole evidently doc\n",
    "evidently_chunks = []\n",
    "\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895ac685-0c13-4ce5-8be8-7a4a846252fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'filename': 'docs-main/api-reference/introduction.mdx',\n",
       " 'section': '## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be3a7d82-de74-4f46-ae64-79e5cac76ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'content': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       " 'filename': 'docs-main/api-reference/introduction.mdx'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a030d-a159-4ec0-b4aa-16cbd576dd81",
   "metadata": {},
   "source": [
    "## Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "This makes sense when:\n",
    "\n",
    "Complex structure: Documents have complex, non-standard structure\n",
    "Semantic coherence: You want chunks that are semantically meaningful\n",
    "Custom logic: You need domain-specific splitting rules\n",
    "Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "\n",
    "You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "You have complex, unstructured documents\n",
    "Quality is more important than cost\n",
    "You have the budget for LLM processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e66c3a-e1c2-4eb4-8416-3c5533f6b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02aca981-bb29-4d92-8b63-3e999790471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c25eb73-ecb8-4b19-8c28-b32197895dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "Use this format:\n",
    "## Section Name\n",
    "Section content with all relevant details\n",
    "---\n",
    "## Another Section Name\n",
    "Another section content\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "# The prompt asks the LLM to:\n",
    "# Split the document logically (not just by length)\n",
    "# Make sections self-contained\n",
    "# Use a specific output format that's easy to parse\n",
    "\n",
    "def llm(prompt, model='gpt-4.1-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4.1-mini', input=messages\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0715cb43-1475-41dc-9b60-beabe1b8e057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Introduction to Regression Testing for LLM Outputs\\nIn this tutorial, you will learn how to perform regression testing for LLM (Large Language Model) outputs. Regression testing helps compare new and old responses after changes in prompts, models, or any system parameters. By re-running the same inputs with updated settings, you can identify significant changes, allowing confident updates or issue fixes.\\n\\n**Note:** This example uses Evidently Cloud. You run evaluations in Python and upload them to the cloud (or view reports locally). For self-hosted setups, replace `CloudWorkspace` with `Workspace`.',\n",
       " \"## Tutorial Scope and Requirements\\n### What You'll Do:\\n- Create a toy Q&A dataset with answers and reference responses.\\n- Generate new answers to the same questions (simulated).\\n- Create and run a Report with Tests using LLM-as-a-judge to evaluate length, correctness, and style consistency.\\n- Build a monitoring Dashboard to track test results over time.\\n\\n### Requirements:\\n- Basic Python knowledge.\\n- OpenAI API key for LLM evaluation.\\n- Evidently Cloud account (sign up free at https://www.evidentlyai.com/register).\\n\\n### Code Access:\\nThe full example is available in a [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) and can be run in [Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n\\n**Note:** The tutorial simulates generating new outputs; it doesn't build an actual LLM app.\",\n",
       " '## 1. Installation and Imports\\nInstall Evidently library with LLM support:\\n```python\\npip install evidently[llm]\\n```\\n\\nImport necessary modules:\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset, DataDefinition\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nConnect to Evidently Cloud:\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n(Optional) Modules for monitoring dashboards as code:\\n```python\\nfrom evidently.ui.dashboards import (DashboardPanelPlot, DashboardPanelTestSuite,\\n                                     DashboardPanelTestSuiteCounter, TestSuitePanelType,\\n                                     ReportFilter, PanelValue, PlotType, CounterAgg)\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nSet your OpenAI API key as an environment variable:\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project in Evidently Cloud\\nConnect to Evidently Cloud with your API token:\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a project within your organization:\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Toy Dataset\\nCreate a dataset with questions and reference answers:\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nPreview dataset with full text display:\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\n### Optional: Data Exploration\\nCheck distribution of text length and sentence counts using Evidently descriptors:\\n```python\\nref_dataset = Dataset.from_pandas(ref_data, data_definition=DataDefinition(),\\n                                 descriptors=[TextLength(\"target_response\", alias=\"Length\"),\\n                                              SentenceCount(\"target_response\", alias=\"Sentence\")])\\nref_dataset.as_dataframe()\\n```\\n\\nGenerate a summary report of the data:\\n```python\\nreport = Report([TextEvals()])\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n```',\n",
       " '## 4. Get New Answers (Simulated)\\nSimulate generating new responses after prompt or model changes by adding a new \"response\" column:\\n```python\\ndata = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times...\"]\\n]\\ncolumns = [\"question\", \"target_response\", \"response\"]\\n\\neval_data = pd.DataFrame(data, columns=columns)\\n```\\n\\n**Connecting with a real app:** Replace this simulated step with your app calls that produce new responses appended to the DataFrame. Evidently\\'s `tracely` library can assist in tracing LLM responses as tabular datasets.',\n",
       " '## 5. Design the Test Suite with LLM-as-a-Judge\\nTo compare new answers with references, define evaluation metrics and tests:\\n\\n### Test Criteria:\\n- **Length Check:** New responses must be <= 200 characters.\\n- **Correctness:** New responses should not contradict the reference answer.\\n- **Style:** New responses should match the style of the reference.\\n\\n### Correctness Judge\\nUse Evidently\\'s `BinaryClassificationPromptTemplate` to create a custom LLM judge for correctness evaluation:\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n    criteria=\"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\nThe ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\nREFERENCE:\\n=====\\n{target_response}\\n=====\"\"\",\\n    target_category=\"incorrect\",\\n    non_target_category=\"correct\",\\n    uncertainty=\"unknown\",\\n    include_reasoning=True,\\n    pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n)\\n```\\n\\n**Tip:** Evaluate and calibrate your judges iteratively as each LLM evaluator behaves like a small ML system.\\n\\n### Style Judge\\nCreate a judge to detect style match focusing on tone, sentence structure, verbosity, and other style-related attributes:\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n    criteria=\"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n    target_category=\"style-mismatched\",\\n    non_target_category=\"style-matching\",\\n    uncertainty=\"unknown\",\\n    include_reasoning=True,\\n    pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n)\\n```\\n\\n**Note:** Style checks are subjective and can be treated as \"non-critical\" tests.',\n",
       " '## 6. Run the Evaluation and Create a Report\\n### Define descriptors to score each response:\\n```python\\ndescriptors = [\\n    LLMEval(\"response\",\\n           template=correctness,\\n           provider=\"openai\",\\n           model=\"gpt-4o-mini\",\\n           alias=\"Correctness\",\\n           additional_columns={\"target_response\": \"target_response\"}),\\n    LLMEval(\"response\",\\n           template=style_match,\\n           provider=\"openai\",\\n           model=\"gpt-4o-mini\",\\n           alias=\"Style\",\\n           additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")\\n]\\n```\\n\\nAdd descriptors to the dataset and preview the scored results:\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\neval_dataset.as_dataframe()\\n```\\n\\n### Define a Report with Tests to validate conditions:\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)])\\n])\\n```\\n\\n- `lte(200)` expects Length ≤ 200 characters.\\n- `eq(0)` expects zero incorrect or style-mismatched responses.\\n- Style mismatch test is non-critical (will show as warning).\\n\\n### Run the Report and upload results to Evidently Cloud:\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nExplore the results in Evidently Platform—metrics summary, test suite results, and scored dataset are available. Tests that fail or warn will be clearly marked for review.\\n\\n**Tip:** Use tags to associate runs with specific parameters like prompt versions.',\n",
       " '## 7. Re-Test After Another Change\\nSimulate another prompt/model change by generating a new set of answers (`eval_data_2`), e.g.:\\n```python\\ndata = [\\n      [\"Why is the sky blue?\", ... , \"The sky looks blue because air molecules scatter the blue light...\"],\\n      [\"How do airplanes stay in the air?\", ... , \"Airplanes fly by generating lift through the wings...\"],\\n      [\"Why do we have seasons?\", ..., \"Seasons change because the distance between the Earth and the sun varies...\"],  # Incorrect response here\\n      [\"How do magnets work?\", ..., \"Magnets operate by creating a magnetic field...\"],\\n      [\"Why does the moon change shape?\", ..., \"The moon\\'s phases occur because we observe varying portions...\"]\\n]\\ncolumns = [\"question\", \"target_response\", \"response\"]\\neval_data_2 = pd.DataFrame(data, columns=columns)\\n```\\n\\nCreate dataset and re-apply evaluation:\\n```python\\neval_dataset_2 = Dataset.from_pandas(eval_data_2, data_definition=DataDefinition())\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\nExplore the new report detecting length issues resolved, but correctness test failing due to contradictions, and style mismatches flagged as non-critical warnings.',\n",
       " '## 8. Build and Use a Monitoring Dashboard\\nTrack test results and trends over time by adding dashboard panels programmatically:\\n\\nAdd panels to show:\\n- Latest test run success rate (counter panel).\\n- Detailed test suite results over time.\\n\\nExample code:\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\n\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\n\\nproject.save()\\n```\\n\\nNavigate to Evidently UI to see test performance summaries, warnings, and failures. Hover or click on tests to explore detailed explanations and reports.\\n\\n**Tip:** Dashboards can also display other metrics over time such as scores or distributions.',\n",
       " '## Conclusion & Next Steps\\nYou have seen how to create a regression testing workflow for LLM outputs, including:\\n- Dataset preparation\\n- Custom LLM-based judges for correctness and style\\n- Automated evaluation with conditional tests\\n- Dashboard monitoring of test results over time\\n\\nThis setup can be integrated into CI/CD pipelines and combined with alerting mechanisms (e.g. email, Slack) for continuous monitoring and quality control of your LLM system outputs.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = intelligent_chunking(evidently_docs[45]['content'])\n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efa4fe-1eff-4d66-907d-4d90a6489a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# evidently_chunks = []\n",
    "# for doc in tqdm(evidently_docs):\n",
    "#     doc_copy = doc.copy()\n",
    "#     doc_content = doc_copy.pop('content')\n",
    "#     sections = intelligent_chunking(doc_content)\n",
    "\n",
    "#     for section in sections:\n",
    "#         section_doc = doc_copy.copy()\n",
    "#         section_doc['section'] = section\n",
    "#         evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e9f6d49-0442-430b-baf0-e88727a3e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLM regression testing',\n",
       " 'description': 'How to run regression testing for LLM outputs.',\n",
       " 'content': 'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.',\n",
       " 'filename': 'docs-main/examples/LLM_regression_testing.mdx'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1f2f50a-8dfa-4418-a941-f04c05f19b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction to Regression Testing for LLM Outputs In this tutorial, you will learn how to\n",
      "perform regression testing for LLM (Large Language Model) outputs. Regression testing helps compare\n",
      "new and old responses after changes in prompts, models, or any system parameters. By re-running the\n",
      "same inputs with updated settings, you can identify significant changes, allowing confident updates\n",
      "or issue fixes.  **Note:** This example uses Evidently Cloud. You run evaluations in Python and\n",
      "upload them to the cloud (or view reports locally). For self-hosted setups, replace `CloudWorkspace`\n",
      "with `Workspace`.\n",
      "\n",
      "---\n",
      "\n",
      "## Tutorial Scope and Requirements ### What You'll Do: - Create a toy Q&A dataset with answers and\n",
      "reference responses. - Generate new answers to the same questions (simulated). - Create and run a\n",
      "Report with Tests using LLM-as-a-judge to evaluate length, correctness, and style consistency. -\n",
      "Build a monitoring Dashboard to track test results over time.  ### Requirements: - Basic Python\n",
      "knowledge. - OpenAI API key for LLM evaluation. - Evidently Cloud account (sign up free at\n",
      "https://www.evidentlyai.com/register).  ### Code Access: The full example is available in a [Jupyter\n",
      "notebook](https://github.com/evidentlyai/community-\n",
      "examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) and can be run in\n",
      "[Colab](https://colab.research.google.com/github/evidentlyai/community-\n",
      "examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).  **Note:** The\n",
      "tutorial simulates generating new outputs; it doesn't build an actual LLM app.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Installation and Imports Install Evidently library with LLM support: ```python pip install\n",
      "evidently[llm] ```  Import necessary modules: ```python import pandas as pd from\n",
      "evidently.future.datasets import Dataset, DataDefinition from evidently.future.descriptors import *\n",
      "from evidently.future.report import Report from evidently.future.presets import TextEvals from\n",
      "evidently.future.metrics import * from evidently.future.tests import * from\n",
      "evidently.features.llm_judge import BinaryClassificationPromptTemplate ```  Connect to Evidently\n",
      "Cloud: ```python from evidently.ui.workspace.cloud import CloudWorkspace ```  (Optional) Modules for\n",
      "monitoring dashboards as code: ```python from evidently.ui.dashboards import (DashboardPanelPlot,\n",
      "DashboardPanelTestSuite,                                      DashboardPanelTestSuiteCounter,\n",
      "TestSuitePanelType,                                      ReportFilter, PanelValue, PlotType,\n",
      "CounterAgg) from evidently.tests.base_test import TestStatus from evidently.renderers.html_widgets\n",
      "import WidgetSize ```  Set your OpenAI API key as an environment variable: ```python import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\" ```\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Create a Project in Evidently Cloud Connect to Evidently Cloud with your API token: ```python\n",
      "ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\") ```  Create a project\n",
      "within your organization: ```python project = ws.create_project(\"Regression testing example\",\n",
      "org_id=\"YOUR_ORG_ID\") project.description = \"My project description\" project.save() ```\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Prepare the Toy Dataset Create a dataset with questions and reference answers: ```python data\n",
      "= [     [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light\n",
      "from the sun more than they scatter red light.\"],     [\"How do airplanes stay in the air?\",\n",
      "\"Airplanes stay in the air because their wings create lift by forcing air to move faster over the\n",
      "top of the wing than underneath, which creates lower pressure on top.\"],     [\"Why do we have\n",
      "seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of\n",
      "the Earth to receive more or less sunlight throughout the year.\"],     [\"How do magnets work?\",\n",
      "\"Magnets work because they have a magnetic field that can attract or repel certain metals, like\n",
      "iron, due to the alignment of their atomic particles.\"],     [\"Why does the moon change shape?\",\n",
      "\"The moon changes shape, or goes through phases, because we see different portions of its\n",
      "illuminated half as it orbits the Earth.\"] ] columns = [\"question\", \"target_response\"]  ref_data =\n",
      "pd.DataFrame(data, columns=columns) ```  Preview dataset with full text display: ```python\n",
      "pd.set_option('display.max_colwidth', None) ref_data.head() ```  ### Optional: Data Exploration\n",
      "Check distribution of text length and sentence counts using Evidently descriptors: ```python\n",
      "ref_dataset = Dataset.from_pandas(ref_data, data_definition=DataDefinition(),\n",
      "descriptors=[TextLength(\"target_response\", alias=\"Length\"),\n",
      "SentenceCount(\"target_response\", alias=\"Sentence\")]) ref_dataset.as_dataframe() ```  Generate a\n",
      "summary report of the data: ```python report = Report([TextEvals()]) my_eval =\n",
      "report.run(ref_dataset, None) my_eval ```\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Get New Answers (Simulated) Simulate generating new responses after prompt or model changes by\n",
      "adding a new \"response\" column: ```python data = [     [\"Why is the sky blue?\",      \"The sky is\n",
      "blue because molecules in the air scatter blue light from the sun more than they scatter red\n",
      "light.\",      \"The sky appears blue because air molecules scatter the sun’s blue light more than\n",
      "they scatter other colors.\"],     [\"How do airplanes stay in the air?\",      \"Airplanes stay in the\n",
      "air because their wings create lift by forcing air to move faster over the top of the wing than\n",
      "underneath, which creates lower pressure on top.\",      \"Airplanes stay airborne because the shape\n",
      "of their wings causes air to move faster over the top than the bottom, generating lift.\"],     [\"Why\n",
      "do we have seasons?\",      \"We have seasons because the Earth is tilted on its axis, which causes\n",
      "different parts of the Earth to receive more or less sunlight throughout the year.\",      \"Seasons\n",
      "occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching\n",
      "different areas as the Earth orbits the sun.\"],     [\"How do magnets work?\",      \"Magnets work\n",
      "because they have a magnetic field that can attract or repel certain metals, like iron, due to the\n",
      "alignment of their atomic particles.\",      \"Magnets generate a magnetic field, which can attract\n",
      "metals like iron by causing the electrons in those metals to align in a particular way, creating an\n",
      "attractive or repulsive force.\"],     [\"Why does the moon change shape?\",      \"The moon changes\n",
      "shape, or goes through phases, because we see different portions of its illuminated half as it\n",
      "orbits the Earth.\",      \"The moon appears to change shape as it orbits Earth, which is because we\n",
      "see different parts of its lit-up half at different times...\"] ] columns = [\"question\",\n",
      "\"target_response\", \"response\"]  eval_data = pd.DataFrame(data, columns=columns) ```  **Connecting\n",
      "with a real app:** Replace this simulated step with your app calls that produce new responses\n",
      "appended to the DataFrame. Evidently's `tracely` library can assist in tracing LLM responses as\n",
      "tabular datasets.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Design the Test Suite with LLM-as-a-Judge To compare new answers with references, define\n",
      "evaluation metrics and tests:  ### Test Criteria: - **Length Check:** New responses must be <= 200\n",
      "characters. - **Correctness:** New responses should not contradict the reference answer. -\n",
      "**Style:** New responses should match the style of the reference.  ### Correctness Judge Use\n",
      "Evidently's `BinaryClassificationPromptTemplate` to create a custom LLM judge for correctness\n",
      "evaluation: ```python correctness = BinaryClassificationPromptTemplate(     criteria=\"\"\"An ANSWER is\n",
      "correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
      "The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes\n",
      "details. REFERENCE: ===== {target_response} =====\"\"\",     target_category=\"incorrect\",\n",
      "non_target_category=\"correct\",     uncertainty=\"unknown\",     include_reasoning=True,\n",
      "pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
      ") ```  **Tip:** Evaluate and calibrate your judges iteratively as each LLM evaluator behaves like a\n",
      "small ML system.  ### Style Judge Create a judge to detect style match focusing on tone, sentence\n",
      "structure, verbosity, and other style-related attributes: ```python style_match =\n",
      "BinaryClassificationPromptTemplate(     criteria=\"\"\"An ANSWER is style-matching when it matches the\n",
      "REFERENCE answer in STYLE, even if the meaning is different. The ANSWER is style-mismatched when it\n",
      "diverges from the REFERENCE answer in STYLE, even if the meaning is the same.  Consider the\n",
      "following STYLE attributes: - tone (friendly, formal, casual, sarcastic, etc.) - sentence structure\n",
      "(simple, compound, complex, etc.) - verbosity level (relative length of answers) - and other similar\n",
      "attributes that may reflect difference in STYLE.  You must focus only on STYLE. Ignore any\n",
      "differences in contents.  ===== {target_response} =====\"\"\",     target_category=\"style-mismatched\",\n",
      "non_target_category=\"style-matching\",     uncertainty=\"unknown\",     include_reasoning=True,\n",
      "pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
      ") ```  **Note:** Style checks are subjective and can be treated as \"non-critical\" tests.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Run the Evaluation and Create a Report ### Define descriptors to score each response:\n",
      "```python descriptors = [     LLMEval(\"response\",            template=correctness,\n",
      "provider=\"openai\",            model=\"gpt-4o-mini\",            alias=\"Correctness\",\n",
      "additional_columns={\"target_response\": \"target_response\"}),     LLMEval(\"response\",\n",
      "template=style_match,            provider=\"openai\",            model=\"gpt-4o-mini\",\n",
      "alias=\"Style\",            additional_columns={\"target_response\": \"target_response\"}),\n",
      "TextLength(\"response\", alias=\"Length\") ] ```  Add descriptors to the dataset and preview the scored\n",
      "results: ```python eval_dataset.add_descriptors(descriptors=descriptors) eval_dataset.as_dataframe()\n",
      "```  ### Define a Report with Tests to validate conditions: ```python report = Report([\n",
      "TextEvals(),     MaxValue(column=\"Length\", tests=[lte(200)]),\n",
      "CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\n",
      "CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]) ]) ```\n",
      "- `lte(200)` expects Length ≤ 200 characters. - `eq(0)` expects zero incorrect or style-mismatched\n",
      "responses. - Style mismatch test is non-critical (will show as warning).  ### Run the Report and\n",
      "upload results to Evidently Cloud: ```python my_eval = report.run(eval_dataset, None)\n",
      "ws.add_run(project.id, my_eval, include_data=True) ```  Explore the results in Evidently\n",
      "Platform—metrics summary, test suite results, and scored dataset are available. Tests that fail or\n",
      "warn will be clearly marked for review.  **Tip:** Use tags to associate runs with specific\n",
      "parameters like prompt versions.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Re-Test After Another Change Simulate another prompt/model change by generating a new set of\n",
      "answers (`eval_data_2`), e.g.: ```python data = [       [\"Why is the sky blue?\", ... , \"The sky\n",
      "looks blue because air molecules scatter the blue light...\"],       [\"How do airplanes stay in the\n",
      "air?\", ... , \"Airplanes fly by generating lift through the wings...\"],       [\"Why do we have\n",
      "seasons?\", ..., \"Seasons change because the distance between the Earth and the sun varies...\"],  #\n",
      "Incorrect response here       [\"How do magnets work?\", ..., \"Magnets operate by creating a magnetic\n",
      "field...\"],       [\"Why does the moon change shape?\", ..., \"The moon's phases occur because we\n",
      "observe varying portions...\"] ] columns = [\"question\", \"target_response\", \"response\"] eval_data_2 =\n",
      "pd.DataFrame(data, columns=columns) ```  Create dataset and re-apply evaluation: ```python\n",
      "eval_dataset_2 = Dataset.from_pandas(eval_data_2, data_definition=DataDefinition())\n",
      "eval_dataset_2.add_descriptors(descriptors=descriptors) my_eval_2 = report.run(eval_dataset_2, None)\n",
      "ws.add_run(project.id, my_eval_2, include_data=True) ```  Explore the new report detecting length\n",
      "issues resolved, but correctness test failing due to contradictions, and style mismatches flagged as\n",
      "non-critical warnings.\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Build and Use a Monitoring Dashboard Track test results and trends over time by adding\n",
      "dashboard panels programmatically:  Add panels to show: - Latest test run success rate (counter\n",
      "panel). - Detailed test suite results over time.  Example code: ```python\n",
      "project.dashboard.add_panel(     DashboardPanelTestSuiteCounter(         title=\"Latest Test run\",\n",
      "filter=ReportFilter(metadata_values={}, tag_values=[]),         size=WidgetSize.FULL,\n",
      "statuses=[TestStatus.SUCCESS],         agg=CounterAgg.LAST,     ),     tab=\"Tests\" )\n",
      "project.dashboard.add_panel(     DashboardPanelTestSuite(         title=\"Test results\",\n",
      "filter=ReportFilter(metadata_values={}, tag_values=[]),         size=WidgetSize.FULL,\n",
      "panel_type=TestSuitePanelType.DETAILED,     ),     tab=\"Tests\" )  project.save() ```  Navigate to\n",
      "Evidently UI to see test performance summaries, warnings, and failures. Hover or click on tests to\n",
      "explore detailed explanations and reports.  **Tip:** Dashboards can also display other metrics over\n",
      "time such as scores or distributions.\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion & Next Steps You have seen how to create a regression testing workflow for LLM\n",
      "outputs, including: - Dataset preparation - Custom LLM-based judges for correctness and style -\n",
      "Automated evaluation with conditional tests - Dashboard monitoring of test results over time  This\n",
      "setup can be integrated into CI/CD pipelines and combined with alerting mechanisms (e.g. email,\n",
      "Slack) for continuous monitoring and quality control of your LLM system outputs.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for sec in sections:\n",
    "    print(textwrap.fill(sec, width=100))\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539247e-19c7-44a2-b0e3-971bb7b61f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
